{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical work : Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Presentation of the Boston Housing dataset\n",
    "\n",
    "This data set concerns the task of predicting housing values in areas of Boston. The used variables and their meanings are as follows:\n",
    "1. CRIM per capita crime rate by town\n",
    "2. ZN proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3. INDUS proportion of non-retail business acres per town\n",
    "4. CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "5. NOX nitric oxides concentration (parts per 10 million)\n",
    "6. RM average number of rooms per dwelling\n",
    "7. AGE proportion of owner-occupied units built prior to 1940\n",
    "8. DIS weighted distances to five Boston employment centres\n",
    "9. RAD index of accessibility to radial highways\n",
    "10. TAX full-value property-tax rate per \\$10,000\n",
    "11. PTRATIO pupil-teacher ratio by town\n",
    "12. LSTAT % lower status of the population\n",
    "13. MEDV Median value of owner-occupied homes in $1000's\n",
    "\n",
    "Source: UCI machine learning repository. (http://www.ics.uci.edu/~mlearn/MLSummary.html).\n",
    "Characteristics: 506 cases; 12 continuous variables\n",
    "Download : housing.tar.gz (11883 bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rendering ++\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"BostonHousing.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the describe method to have basic stats about the data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Plots to visualize data and observe correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(data[\"medv\"], bins=20)\n",
    "plt.xlabel('price ($1000s)')\n",
    "plt.ylabel('count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=data.keys()[:-1]\n",
    "for u in feature_names:\n",
    "    fig = data.plot(x=u, y=\"medv\", style=\"o\")\n",
    "    fig.set_ylabel(\"medv\", fontsize=15)\n",
    "    fig.set_xlabel(u, fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - Simple linear regression with 1 feature\n",
    "\n",
    "We use the \"StatsModels\" Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"lstat\"]\n",
    "X = sm.add_constant(X)  # add the intercept term => necessary with statsmodels regression\n",
    "y = data[\"medv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = sm.OLS(y, X).fit()\n",
    "ols.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "\n",
    "* p-values for the Student test on the coefficients are negligible: the coefficients cannot be considered to be zero.\n",
    "We recall that to test the nullity of the coefficients, $H_0$ corresponds to $\\beta_i=0$. Therefore, a very low $p$-value leads to reject $H_0$.\n",
    "\n",
    "* Coefficient of determination : \n",
    "$R^{2}=1-\\frac{\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}}{\\sum_{i=1}^{n}\\left(y_{i}-\\overline{y}\\right)^{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing the regression line on top of the scatterplot\n",
    "fig = data.plot(x=\"lstat\", y=\"medv\", style=\"o\")\n",
    "fig.set_ylabel(\"medv\")\n",
    "ypred = ols.predict(X)\n",
    "fig.plot(data['lstat'], ypred, 'r', linewidth=2.5)\n",
    "fig.set_title('R^2 = {:.2f}'.format(ols.rsquared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y - ypred\n",
    "fig = sm.qqplot(residuals, dist=\"norm\", line=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Normal distributions of the error terms in the regression model, the residual would be distributed close to the red line. Therefore, here it's difficult to validate the asssumption of normal residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Linear regression with 2 features, lstat and age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[[\"age\", \"lstat\"]]\n",
    "X = sm.add_constant(X)  # add the intercept term => necessary with statsmodel regression\n",
    "y = data[\"medv\"]\n",
    "ols2 = sm.OLS(y, X).fit()\n",
    "ols2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "\n",
    "* AIC=3281, while it was 3287 with only 2 parameters (1 feature): the model with 3 parameters (2 features) is better.\n",
    "\n",
    "* Again, $p$-values are really low and the parameters cannot be considered to be zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 - Linear regression with all features and model selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the features are all columns but the last one\n",
    "features = data.keys()[:-1]\n",
    "X = data[features]\n",
    "X = sm.add_constant(X)  # add the intercept term => necessary to\n",
    "y = data[\"medv\"]\n",
    "ols3 = sm.OLS(y, X).fit()\n",
    "ols3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "\n",
    "* AIC=3036, the model with all features is better.\n",
    "\n",
    "* $p$-values for the parameters associated to 'indus' and 'age' are high, which suggests that the coefficients should be set to 0. However, we usually do it sequentially, first by considering the model without indus, checking again the $p$-values and iterate if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use all features but 'indus'\n",
    "features = features.drop('indus')\n",
    "X = data[features]\n",
    "X = sm.add_constant(X)  # add the intercept term => necessary to\n",
    "y = data[\"medv\"]\n",
    "ols3 = sm.OLS(y, X).fit()\n",
    "ypred=ols3.predict(X)\n",
    "ols3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y - ypred\n",
    "fig = sm.qqplot(residuals, dist=\"norm\", line=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly better in terms of residuals normality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "\n",
    "* AIC=3034, the model with all features but indus is better than the previous one.\n",
    "\n",
    "* $p$-values for the parameters 'age' is still high, which suggests that the coefficients should be set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use all features but 'indus'\n",
    "features = features.drop('age')\n",
    "X = data[features]\n",
    "X = sm.add_constant(X)  # add the intercept term\n",
    "y = data[\"medv\"]\n",
    "ols4 = sm.OLS(y, X).fit()\n",
    "ols4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "\n",
    "* AIC=3032, the model with all features but indus and age is better than the previous ones.\n",
    "\n",
    "* $p$-values for all parameters are negligible: it does not seem possible ro reduce the model furthermore a priori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 - Prediction Test\n",
    "\n",
    "A classical way to evaluate a prediction function in statistical learning, is to spare one part of the dataset for testing purpose (classically 20%). The dataset is thus split into a \"training set\" and a \"test set\". The rule to never forget is that in such configuration, the test set should not be used at all in the training process (notably for the selection of hyperparameters): it is used only to assess the prediction capacity of a fully trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "ols5 = sm.OLS(y_train, X_train).fit()\n",
    "ols5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = ols5.predict(X_test)\n",
    "expected = y_test\n",
    "\n",
    "plt.scatter(expected, predicted)\n",
    "plt.plot([0, 50], [0, 50], '--k')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('True price ($1000s)')\n",
    "plt.ylabel('Predicted price ($1000s)')\n",
    "print(\"RMS: {:.2f}\".format(np.sqrt(np.mean((predicted - expected) ** 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 - Regularization\n",
    "\n",
    "We explore the elastic-net method, which is a combination of Lasso and Ridge-Penalty:\n",
    "\n",
    "$\\hat{\\beta} \\equiv \\underset{\\beta}{\\operatorname{argmin}}\\frac{1}{2N}\\|y-X \\beta\\|^{2}+\\alpha\\left((1-\\lambda_1)\\frac{\\displaystyle|\\beta\\|^{2}}{2}+\\lambda_{1}\\|\\beta\\|_{1}\\right)$\n",
    "\n",
    "Be careful: we need to standardize the input data !\n",
    "\n",
    "We use scikitlearn for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.keys()[:-1]\n",
    "X = data[features]\n",
    "y = data[\"medv\"]\n",
    "\n",
    "#we standardize X\n",
    "from sklearn import preprocessing\n",
    "scaler_x= preprocessing.StandardScaler().fit(X)\n",
    "X_std=scaler_x.transform(X)\n",
    "\n",
    "y_centered=y-np.mean(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.a - The Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Lasso Regression Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "als=np.logspace(-3, 1.5, 200)\n",
    "coefs=[]\n",
    "for al in als:\n",
    "    reg = ElasticNet(alpha=al,l1_ratio=1)\n",
    "    reg.fit(X_std, y_centered)\n",
    "    coefs.append(reg.coef_)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of different colors to draw our coefficients\n",
    "colormap = plt.cm.gist_ncar \n",
    "colorst = [colormap(i) for i in np.linspace(0, 0.95,len(features))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(features)):\n",
    "    plt.plot(np.log10(als), [x[i] for x in coefs], label=features[i], color=colorst[i])\n",
    "plt.xlabel(r'$\\ln(\\alpha)$')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('Lasso Regression Path')\n",
    "plt.legend(bbox_to_anchor=(1.04,0.5), loc=\"center left\", borderaxespad=0)\n",
    "plt.axis('tight')\n",
    "#ymin, ymax = plt.ylim()\n",
    "#plt.axis([0,0.2,-1,1],'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Estimation of the Generalization Error by Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "features=list(data)[0:-1]\n",
    "X = data[features]\n",
    "scaler_x= preprocessing.StandardScaler().fit(X)\n",
    "X_std=scaler_x.transform(X)\n",
    "\n",
    "ols = LinearRegression(fit_intercept=False)\n",
    "score_ols = cross_val_score(ols, X_std, y_centered, cv=5, \n",
    "                            scoring=('neg_mean_squared_error'))\n",
    "print(\"Neg MSE for the full model:\",np.mean(score_ols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = GridSearchCV(ElasticNet(l1_ratio=1, fit_intercept=False), \n",
    "                   param_grid={'alpha': np.logspace(-1.5, -0.5, 500)}, \n",
    "                   cv=5, \n",
    "                   scoring='neg_mean_squared_error')\n",
    "clf.fit(X_std, y_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log10(clf.param_grid['alpha']), \n",
    "         clf.cv_results_['mean_test_score'])\n",
    "plt.xlabel(r'$\\ln(\\alpha)$')\n",
    "plt.ylabel('Neg MSE')\n",
    "plt.title('Evolution of the Cross Validation Error with the penalty '\\\n",
    "          'coefficient for the Lasso estimator')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = clf.best_params_['alpha']\n",
    "print(\"best alpha:\", best_alpha,\n",
    "      \"\\n\",\"best MSE:\", clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = ElasticNet(alpha=best_alpha, l1_ratio=1)\n",
    "reg.fit(X_std, y)\n",
    "for i in range(len(features)):\n",
    "    print(\"coeff {} : {:.2f}\".format(features[i], reg.coef_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one coefficient is zero: that of 'age'.\n",
    "We perform an Ordinary Linear Regression (without the penalty term) and by setting to 0 the coefficient $\\beta_{age}$: the idea is to check whether we perform better by removing the bias due to the penalty term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=list(data)[0:-1]\n",
    "features.remove(\"age\")\n",
    "X = data[features]\n",
    "#we standardize X\n",
    "scaler_x= preprocessing.StandardScaler().fit(X)\n",
    "X_std=scaler_x.transform(X)\n",
    "\n",
    "ols = LinearRegression(fit_intercept=False)\n",
    "score_ols = cross_val_score(ols, X_std, y_centered, cv=5, \n",
    "                            scoring=('neg_mean_squared_error'))\n",
    "\n",
    "print(\"MSE for the model with all features but age and indus:\",np.mean(score_ols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is indeed slightly better. Since with the original Ordinary Least-Square regression, the model without \"indus\" and \"age\" was pretty good, we also compute the Cross Validation Error for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=list(data)[0:-1]\n",
    "features.remove(\"age\")\n",
    "features.remove(\"indus\") #in addition to \"age\", we also remove \"indus\"\n",
    "X = data[features]\n",
    "#we standardize X\n",
    "scaler_x= preprocessing.StandardScaler().fit(X)\n",
    "X_std=scaler_x.transform(X)\n",
    "\n",
    "ols = LinearRegression(fit_intercept=False)\n",
    "score_ols = cross_val_score(ols, X_std, y_centered, cv=5, scoring=('neg_mean_squared_error'))\n",
    "print(\"MSE for the model with all features but age and indus:\",np.mean(score_ols))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is indeed better. The strategy set in place with the lasso failed to find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idem for the ridge regression\n",
    "als=np.logspace(-2, 5, 100)\n",
    "coefs=[]\n",
    "for al in als:\n",
    "    reg = ElasticNet(alpha=al,l1_ratio=0,fit_intercept=False) \n",
    "    reg.fit(X_std, y)\n",
    "    coefs.append(reg.coef_)\n",
    "for i in range(len(features)):\n",
    "    plt.plot(np.log10(als), [x[i] for x in coefs], label=features[i], color=colorst[i])\n",
    "plt.xlabel(r'$\\ln(\\alpha)$')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('Ridge Regression Path')\n",
    "l3 = plt.legend(bbox_to_anchor=(1.04,0.5), loc=\"center left\", borderaxespad=0)\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "features=list(data)[0:-1]\n",
    "X = data[features]\n",
    "scaler_x= preprocessing.StandardScaler().fit(X)\n",
    "X_std=scaler_x.transform(X)\n",
    "y_centered=y-np.mean(y)\n",
    "\n",
    "clf = GridSearchCV(ElasticNet(l1_ratio=0, fit_intercept=False), \n",
    "                   param_grid={'alpha': np.logspace(-2, 2, 200)}, \n",
    "                   cv=5, \n",
    "                   scoring='neg_mean_squared_error')\n",
    "clf.fit(X_std, y_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log10(clf.param_grid['alpha']), clf.cv_results_['mean_test_score'])\n",
    "plt.xlabel(r'$\\ln(\\alpha)$')\n",
    "plt.ylabel('Neg MSE')\n",
    "plt.title('Evolution of the Cross Validation Error with the penalty '\\\n",
    "          'coefficient for the ridge estimator')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha=clf.best_params_['alpha']\n",
    "print(\"best alpha:\", best_alpha,\"\\n\",\"best MSE:\",clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ridge regression provides the best configuration between ordinary least-square, Lasso and ridge, at least in terms of Cross Validation prediction error, which can be seen as an approximation of the generalization error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8 - Back to Data Visualization\n",
    "\n",
    "One reason for the relatively disappointing performance of Lasso in this case is the correlations between features. Ridge is known to perform better in this situation. \n",
    "\n",
    "We can visualize the correlations of the data with several tools. The firs one is the scattermatrix: it shows the pairwise scatter plots for all the covariates (as well as the target for the graph below).\n",
    "\n",
    "We can use directly a function from the pandas module (note that the matrix of plots is symmetrical; on the diagonal, we show the histogram of the variables. (Be patient, it takes a litlle time to proceed....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 12))\n",
    "sns.heatmap(data.corr(), cmap='seismic', annot=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grr = pd.plotting.scatter_matrix(data, c='darkorchid', \n",
    "                                 figsize=(15, 15), marker='o',\n",
    "                                 hist_kwds={'bins': 20, 'color' : 'darkmagenta'}, \n",
    "                                 s=10, alpha=.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possibility would be to use the seaborn package, which provides nice data visualization tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the correlation matrix can help us summarize all this information. \n",
    "We use great customized tool for this purpose !\n",
    "\n",
    "Strong correlations between features appear strikingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Thanks to: https://github.com/drazenz/heatmap/blob/master/heatmap.py\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def heatmap(x, y, **kwargs):\n",
    "    if 'color' in kwargs:\n",
    "        color = kwargs['color']\n",
    "    else:\n",
    "        color = [1]*len(x)\n",
    "\n",
    "    if 'palette' in kwargs:\n",
    "        palette = kwargs['palette']\n",
    "        n_colors = len(palette)\n",
    "    else:\n",
    "        n_colors = 256 # Use 256 colors for the diverging color palette\n",
    "        palette = sns.color_palette(\"Blues\", n_colors) \n",
    "\n",
    "    if 'color_range' in kwargs:\n",
    "        color_min, color_max = kwargs['color_range']\n",
    "    else:\n",
    "        color_min, color_max = min(color), max(color) # Range of values that will be mapped to the palette, i.e. min and max possible correlation\n",
    "\n",
    "    def value_to_color(val):\n",
    "        if color_min == color_max:\n",
    "            return palette[-1]\n",
    "        else:\n",
    "            val_position = float((val - color_min)) / (color_max - color_min) # position of value in the input range, relative to the length of the input range\n",
    "            val_position = min(max(val_position, 0), 1) # bound the position betwen 0 and 1\n",
    "            ind = int(val_position * (n_colors - 1)) # target index in the color palette\n",
    "            return palette[ind]\n",
    "\n",
    "    if 'size' in kwargs:\n",
    "        size = kwargs['size']\n",
    "    else:\n",
    "        size = [1]*len(x)\n",
    "\n",
    "    if 'size_range' in kwargs:\n",
    "        size_min, size_max = kwargs['size_range'][0], kwargs['size_range'][1]\n",
    "    else:\n",
    "        size_min, size_max = min(size), max(size)\n",
    "\n",
    "    size_scale = kwargs.get('size_scale', 500)\n",
    "\n",
    "    def value_to_size(val):\n",
    "        if size_min == size_max:\n",
    "            return 1 * size_scale\n",
    "        else:\n",
    "            val_position = (val - size_min) * 0.99 / (size_max - size_min) + 0.01 # position of value in the input range, relative to the length of the input range\n",
    "            val_position = min(max(val_position, 0), 1) # bound the position betwen 0 and 1\n",
    "            return val_position * size_scale\n",
    "    if 'x_order' in kwargs: \n",
    "        x_names = [t for t in kwargs['x_order']]\n",
    "    else:\n",
    "        x_names = [t for t in sorted(set([v for v in x]))]\n",
    "    x_to_num = {p[1]:p[0] for p in enumerate(x_names)}\n",
    "\n",
    "    if 'y_order' in kwargs: \n",
    "        y_names = [t for t in kwargs['y_order']]\n",
    "    else:\n",
    "        y_names = [t for t in sorted(set([v for v in y]))]\n",
    "    y_to_num = {p[1]:p[0] for p in enumerate(y_names)}\n",
    "\n",
    "    plot_grid = plt.GridSpec(1, 15, hspace=0.2, wspace=0.1) # Setup a 1x10 grid\n",
    "    ax = plt.subplot(plot_grid[:,:-1]) # Use the left 14/15ths of the grid for the main plot\n",
    "\n",
    "    marker = kwargs.get('marker', 's')\n",
    "\n",
    "    kwargs_pass_on = {k:v for k,v in kwargs.items() if k not in [\n",
    "         'color', 'palette', 'color_range', 'size', 'size_range', 'size_scale', 'marker', 'x_order', 'y_order'\n",
    "    ]}\n",
    "\n",
    "    ax.scatter(\n",
    "        x=[x_to_num[v] for v in x],\n",
    "        y=[y_to_num[v] for v in y],\n",
    "        marker=marker,\n",
    "        s=[value_to_size(v) for v in size], \n",
    "        c=[value_to_color(v) for v in color],\n",
    "        **kwargs_pass_on\n",
    "    )\n",
    "    ax.set_xticks([v for k,v in x_to_num.items()])\n",
    "    ax.set_xticklabels([k for k in x_to_num], rotation=45, horizontalalignment='right')\n",
    "    ax.set_yticks([v for k,v in y_to_num.items()])\n",
    "    ax.set_yticklabels([k for k in y_to_num])\n",
    "\n",
    "    ax.grid(False, 'major')\n",
    "    ax.grid(True, 'minor')\n",
    "    ax.set_xticks([t + 0.5 for t in ax.get_xticks()], minor=True)\n",
    "    ax.set_yticks([t + 0.5 for t in ax.get_yticks()], minor=True)\n",
    "\n",
    "    ax.set_xlim([-0.5, max([v for v in x_to_num.values()]) + 0.5])\n",
    "    ax.set_ylim([-0.5, max([v for v in y_to_num.values()]) + 0.5])\n",
    "    ax.set_facecolor('#F1F1F1')\n",
    "\n",
    "    # Add color legend on the right side of the plot\n",
    "    if color_min < color_max:\n",
    "        ax = plt.subplot(plot_grid[:,-1]) # Use the rightmost column of the plot\n",
    "\n",
    "        col_x = [0]*len(palette) # Fixed x coordinate for the bars\n",
    "        bar_y=np.linspace(color_min, color_max, n_colors) # y coordinates for each of the n_colors bars\n",
    "\n",
    "        bar_height = bar_y[1] - bar_y[0]\n",
    "        ax.barh(\n",
    "            y=bar_y,\n",
    "            width=[5]*len(palette), # Make bars 5 units wide\n",
    "            left=col_x, # Make bars start at 0\n",
    "            height=bar_height,\n",
    "            color=palette,\n",
    "            linewidth=0\n",
    "        )\n",
    "        ax.set_xlim(1, 2) # Bars are going from 0 to 5, so lets crop the plot somewhere in the middle\n",
    "        ax.grid(False) # Hide grid\n",
    "        ax.set_facecolor('white') # Make background white\n",
    "        ax.set_xticks([]) # Remove horizontal ticks\n",
    "        ax.set_yticks(np.linspace(min(bar_y), max(bar_y), 3)) # Show vertical ticks for min, middle and max\n",
    "        ax.yaxis.tick_right() # Show vertical ticks on the right \n",
    "\n",
    "\n",
    "def corrplot(data, size_scale=280, marker='s'):\n",
    "    corr = pd.melt(data.reset_index(), id_vars='index')\n",
    "    corr.columns = ['x', 'y', 'value']\n",
    "    heatmap(\n",
    "        corr['x'], corr['y'],\n",
    "        color=corr['value'], color_range=[-1, 1],\n",
    "        palette=sns.diverging_palette(20, 220, n=256),\n",
    "        size=corr['value'].abs(), size_range=[0,1],\n",
    "        marker=marker,\n",
    "        x_order=data.columns,\n",
    "        y_order=data.columns[::-1],\n",
    "        size_scale=size_scale\n",
    "    )\n",
    "    \n",
    "plt.figure(figsize=(8,8))\n",
    "corrplot(data.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for today ...\n",
    "\n",
    "Or quite ...\n",
    "\n",
    "A last exercise for you:\n",
    "\n",
    "We selected the hyper-parameters for the Lasso and Ridge estimators respectively. The interest of the elastic net is that it sometimes can combine the advantages of both. \n",
    "\n",
    "Define a 2D grid search for both $\\alpha$ and $\\lambda_1$ and select the 2 best hyperparameters by cross-validation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(ElasticNet(fit_intercept=False), \n",
    "                  param_grid={'alpha': np.logspace(-1.5, 0.5, 50), \n",
    "                             'l1_ratio':  np.logspace(-1.5, 0.5, 50)})\n",
    "clf.fit(X_std, y_centered)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
