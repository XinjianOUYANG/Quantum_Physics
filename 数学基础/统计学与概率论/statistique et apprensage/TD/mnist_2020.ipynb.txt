{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to representation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea of representation learning is to transform data at our disposal and find a new representation of it in a new embedded space.\n",
    "This new representation may be useful for several reasons, see the excellent article by Bengio et al. for more detailed explanations: https://arxiv.org/pdf/1206.5538.pdf.\n",
    "Representations can then be used for a variety of machine learning tasks both supervised (such as prediction or classification) or unsupervised (such as clustering).\n",
    "\n",
    "In this notebook, we are going to introduce basic ideas and methods behind representation learning, applied to the MNIST dataset.\n",
    "It is a dataset that is made of images of handwritten digits and where each image is associated to the actual digit it represents.\n",
    "Note that representation learning is by no means restricted to image datasets.\n",
    "The latter are nonetheless particularly suited to illustrating techniques of representation learning as we can visualize the original data and their reconstruction (to be detailed below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are loading data from the MNIST dataset, it is one of the standard datasets used in `keras`, a handy Python framework for deep learning.\n",
    "Each sample is an image, i.e. a matrix of size $n \\times n = 28 \\times 28 = 784$.\n",
    "The set of all such matrices is denoted $X^{mat} \\in \\mathbb{R}^{n_{samples} \\times n \\times n}$, where $n_{samples}$ denotes the number of images available in the dataset.\n",
    "Each image is associated to a label denoting the digit to which the image corresponds.\n",
    "The set of all such labels is denoted $l \\in \\mathbb{R}^{n_{samples}}$.\n",
    "\n",
    "As is standard in machine learning, the whole dataset must be split between a training dataset, used to train algorithms, and a test dataset, on which we evaluate the performance of our algorithms.\n",
    "This is done automatically by the `keras` function `mnist.load_data()`.\n",
    "\n",
    "We therefore load the MNIST dataset and split it as $(X^{mat}_{train}, l_{train})$ and $(X^{mat}_{test}, l_{test})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "(X_mat_train, l_train), (X_mat_test, l_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking shapes of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first investigate:\n",
    "- how the train and test sets have been split,\n",
    "- what the dimensions of the data we loaded are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_train = X_mat_train.shape[0]\n",
    "n_samples_test = X_mat_test.shape[0]\n",
    "n_samples = n_samples_train + n_samples_test\n",
    "n = X_mat_train.shape[1]\n",
    "print('n_samples_train / n_samples = {} / {} = {:.2f}'.format(n_samples_train, \n",
    "                                                              n_samples, \n",
    "                                                              n_samples_train/n_samples))\n",
    "print('n_samples_train / n_samples = {} / {} = {:.2f}'.format(n_samples_test, \n",
    "                                                              n_samples, \n",
    "                                                              n_samples_test/n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape X_mat_train:\", X_mat_train.shape)\n",
    "print(\"Shape l_train:\", l_train.shape)\n",
    "print(\"Shape X_mat_test:\", X_mat_test.shape)\n",
    "print(\"Shape l_test:\", l_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing data as matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display 10 images of the training dataset to explore the latter.\n",
    "All figures are displayed using `matplotlib.pyplot`. We also use a very convenient module `matplotlib.gridspec` which specifies how subplots of a figure are organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 10\n",
    "fig = plt.figure(figsize = (12, 2))\n",
    "gs = gridspec.GridSpec(nrows = 1, ncols = n_examples, left = 0.1, bottom = 0.25, \n",
    "                       right = 0.95, top = 0.95, wspace = 0.0, hspace = 0.0)\n",
    "for i in range(0, n_examples):\n",
    "    ax1 = plt.subplot(gs[i])\n",
    "    plt.imshow(X_mat_train[i,:,:], cmap = \"gray\")\n",
    "    ax1.set(adjustable = \"datalim\", aspect = \"auto\")\n",
    "    ax1.get_xaxis().set_visible(False)\n",
    "    ax1.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting each matrix into a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are not going to take advantage of the 2 dimensional structure of the data.\n",
    "Instead, we will convert all matrices of size $n \\times n$ into vectors of size $d = n^2$.\n",
    "Furtheremore, components of an image are originally comprised between 0 and 255; we want to work with data between 0 and 1 and thus normalize accordingly.\n",
    "\n",
    "We obtain $X_{train} \\in [0,1]^{n_{samples,train} \\times d}$ and $X_{test} \\in [0,1]^{n_{samples,test} \\times d}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = n * n\n",
    "X_train = X_mat_train.reshape(n_samples_train, d) / 255.\n",
    "X_test = X_mat_test.reshape(n_samples_test, d) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape X_train:\", X_train.shape)\n",
    "print(\"Shape l_train:\", l_train.shape)\n",
    "print(\"Shape X_test:\", X_test.shape)\n",
    "print(\"Shape l_test:\", l_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle of representation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample $x \\in \\mathbb{R}^d$ of our dataset will be represented in a new space as $z \\in \\mathbb{R}^h$, with $h < d$, and will then be reconstructed in the original space as $y \\in \\mathbb{R}^d$ with some approximation, which is assessed by the reconstruction error $\\epsilon(x) = \\frac{1}{d} \\sum_{j=1}^d | x_j - y_j |^2$.\n",
    "\n",
    "The whole process can be summed up as:\n",
    "\n",
    "$x \\in \\mathbb{R}^d \\rightarrow z = f(x) \\in \\mathbb{R}^h \\rightarrow y = g(f(x)) \\in \\mathbb{R}^d$\n",
    "\n",
    "or matrix-wise:\n",
    "\n",
    "$X \\in \\mathbb{R}^{n_{samples} \\times d} \\rightarrow Z = f(X) \\in \\mathbb{R}^{n_{samples} \\times h} \\rightarrow Y = g(f(X)) \\in \\mathbb{R}^{n_{samples} \\times d}$\n",
    "\n",
    "Learning the representation in the new embedded space $\\mathbb{R}^{h}$ will be driven by how well we can reconstruct the original data $X$ as $Y$ by _going through_ the embedded space, i.e. the representation will be learnt by minimizing the reconstruction error over the whole dataset:\n",
    "\n",
    "<center>$\\epsilon(X) = \\frac{1}{n_{samples}} \\sum_{i=1}^{n_{samples}} \\frac{1}{d} \\sum_{j=1}^d | X_{ij} - Y_{ij} |^2$.</center>\n",
    "\n",
    "Specifying how we choose the functions $f$ and $g$ constitutes a model of representation learning. In the following, we will focus on Principal Component Analysis and Autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis is one of the simplest technique to represent data in a space of reduced dimension.\n",
    "Basically, it transforms the data to a new coordinate system such that the greatest variance by some projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.\n",
    "If we choose to keep only the first $h$ dimensions of this new coordinate system, we have a new representation of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing PCA by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We will perform a PCA on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train = X_train.mean(axis = 0)\n",
    "print(\"Shape m_train: \", m_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector $m$ is the mean vector of each pixel. A PCA needs to be applied on a matrix with mean zero, we therefore define $X^0_{train}$ as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0_train = X_train - m_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform an SVD decomposition of this centered matrix as $X^0_{train} = U \\mathrm{Diag}(s) V^T$, with:\n",
    "- $X^0_{train} \\in \\mathbb{R}^{n_{train} \\times d}$,\n",
    "- $U \\in \\mathbb{R}^{n_{train} \\times n_{train}}$,\n",
    "- $Diag(s) \\in \\mathbb{R}^{n_{train} \\times d}$,\n",
    "- $V \\in \\mathbb{R}^{d \\times d}$.\n",
    "\n",
    "An implementation of the SVD decomposition is available via the infamous `numpy` library.\n",
    "For numerical efficiency, we set `full_matrices = False` in `np.linalg.svd`, which will reduce their dimensions to:\n",
    "- $U \\in \\mathbb{R}^{n_{train} \\times d}$,\n",
    "- $Diag(s) \\in \\mathbb{R}^{d \\times d}$,\n",
    "- $V \\in \\mathbb{R}^{d \\times d}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "U, s, V = np.linalg.svd(X0_train, full_matrices = False)\n",
    "print(\"Shape U:\", U.shape)\n",
    "print(\"Shape s:\", s.shape)\n",
    "print(\"Shape V:\", V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the singular values :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(s)\n",
    "ax.set_xlabel('Singular value #')\n",
    "ax.set_ylabel('Singular value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot gives a hint on the number of singular values needed to have a proper tradeoff between compression and data fidelity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the representation of our data in a new space of dimension $h$ by truncating the matrix $V$ and computing:\n",
    "\n",
    "$Z^0_{train} = f(X^0_{train}) = X^0_{train} V_h^T \\in \\mathbb{R}^{n \\times h}$,\n",
    "\n",
    "where $V_h \\in \\mathbb{R}^{h\\times d}$. Let's start with a small representation of $h = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 2\n",
    "Vh = V[:h]\n",
    "Z0_train = np.dot(X0_train, Vh.transpose())\n",
    "print(\"Shape Z0_train:\", Z0_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute the reconstruction of our data from the representation space:\n",
    "\n",
    "$Y^0_{train} = g(Z^0_{train}) = Z^0_{train} V_h$,\n",
    "\n",
    "i.e.:\n",
    "\n",
    "$Y^0_{train} = g(f(X^0_{train})) = X^0_{train} V_h^T V_h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0_train = np.dot(Z0_train, Vh)\n",
    "print(\"Shape Y0_train:\", Y0_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us not forget that we have computed the PCA of a zero mean matrix. We add back the mean of our data $m_{train}$ to obtain the final reconstruction $Y_{train}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y0_train + m_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also reconstruct the test set as well, using the PCA matrices computed with the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_test = X_test.mean(axis = 0)\n",
    "m_test = m_train\n",
    "X0_test = X_test - m_test\n",
    "Z0_test = np.dot(X0_test, Vh.transpose())\n",
    "Y0_test = np.dot(Z0_test, Vh)\n",
    "Y_test = Y0_test + m_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compute the average reconstruction error on our data, for both the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_pca_train = np.sum((X_train - Y_train) ** 2) / d / n_samples_train\n",
    "e_pca_test = np.sum((X_test - Y_test) ** 2) / d / n_samples_test\n",
    "print(\"Homemade PCA training reconstruction error with h =\", h, \"PCs:\", \n",
    "      str(round(e_pca_train, 3)))\n",
    "print(\"Homemade PCA test reconstruction error with h =\", h, \"PCs:\", \n",
    "      str(round(e_pca_test, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the method in the `metrics` module from `sklearn`, which gives the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_pca_train = mean_squared_error(X_train, Y_train)\n",
    "e_pca_test = mean_squared_error(X_test, Y_test)\n",
    "print(\"Homemade PCA training reconstruction error with h =\", h, \"PCs:\", \n",
    "      str(round(e_pca_train, 3)))\n",
    "print(\"Homemade PCA test reconstruction error with h =\", h, \"PCs:\", \n",
    "      str(round(e_pca_test, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing PCA using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is rather important to understand how a PCA works behind the scenes, which is why we implemented it almost from scratch.\n",
    "A PCA can nevertheless be computed automatically using `scikit-learn` and its `decomposition` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "h = 2\n",
    "pca = PCA(n_components = h)\n",
    "pca.fit(X0_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notably check what the variance of our data in the new coordinate system along each axis is by printing `pca.explained_variance_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Explained variance for axes 1 and 2:\", pca.explained_variance_)\n",
    "print(\"Ratios of explained variance for axes 1 and 2:\", \n",
    "      pca.explained_variance_ratio_)\n",
    "print(\"Total ratio of explained variance of the data in this new space:\", \n",
    "      np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furtheremore, `pca.transform` plays the role of the projection operator $V_h$ that we have introduced earlier. The application of $V_h^T$ is done through `pca.inverse_transform`\n",
    "We can therefore reconstruct the data and compute the reconstruction error which is, unsurprisingly, the same as the one computed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z0_train = pca.transform(X0_train)\n",
    "Y0_train = pca.inverse_transform(Z0_train)\n",
    "Y_train = Y0_train + m_train\n",
    "e_pca_train = mean_squared_error(X_train, Y_train)\n",
    "print(\"SKL PCA reconstruction error with h =\", h, \"PCs:\", str(round(e_pca_train, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentioned, learning a representation of our data in a space of reduced dimension and reconstructing it in the original space can be done for various kinds of data.\n",
    "However, one of the benefits of working with images is that we can visualize the reconstructed data and compare it to the original ones.\n",
    "This is not always possible and it can be much harder to assess how well we have reconstructed data.\n",
    "\n",
    "Let us see the reconstruction of our data in this case. We first need to convert our reconstructed vectors to matrices to display images, we hence define $Y^{mat}_{train} \\in \\mathbb{R}^{n_{samples} \\times n \\times n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_mat_train = Y_train.reshape(60000, 28, 28)\n",
    "n_examples = 10\n",
    "fig = plt.figure(figsize = (12, 4))\n",
    "gs = gridspec.GridSpec(nrows = 2, ncols = n_examples, left = 0.1, \n",
    "                       bottom = 0.25, right = 0.95, top = 0.95,\n",
    "                       wspace = 0.0, hspace = 0.0)\n",
    "for i in range(0, n_examples):\n",
    "    ax1 = plt.subplot(gs[i])\n",
    "    plt.imshow(X_mat_train[i,:,:], cmap = \"gray\")\n",
    "    ax1.set(adjustable = \"datalim\", aspect = \"auto\")\n",
    "    ax1.get_xaxis().set_visible(False)\n",
    "    ax1.get_yaxis().set_visible(False)\n",
    "    ax2 = plt.subplot(gs[n_examples+i])\n",
    "    plt.imshow(Y_mat_train[i,:,:], cmap = \"gray\")\n",
    "    ax2.set(adjustable = \"datalim\", aspect = \"auto\")\n",
    "    ax2.get_xaxis().set_visible(False)\n",
    "    ax2.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reconstruction is not particularly good with $h = 2$ only.\n",
    "There is actually nothing very surprising since the dimension of the representation space is very low, probably too low to explain all the information and variability of our data.\n",
    "\n",
    "Let's try with several values of $h$ to see how the results change.\n",
    "We are going to store all our results in dictionaries for convenience and reusability.\n",
    "This is particularly important when working with lots of different models (it will be even more crucial for autoencoders).\n",
    "\n",
    "Obviously, the maximum value for $h$ is $d$, that of the data itself.\n",
    "\n",
    "For each value of $h$, we compute the PCA, the reconstruction error, the total ratio of variance explained, and we also display both the original image and the associated reconstruction image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pca = {}\n",
    "dict_e_pca_train = {}\n",
    "dict_var_exp_train = {}\n",
    "\n",
    "for h in [2, 5, 10, 20, 50, 100, 784]:\n",
    "    dict_pca[h] = PCA(n_components = h)\n",
    "    dict_pca[h].fit(X0_train)\n",
    "    pca = dict_pca[h]\n",
    "    Z0_train = pca.transform(X0_train)\n",
    "    Y0_train = pca.inverse_transform(Z0_train)\n",
    "    Y_train = Y0_train + m_train\n",
    "    e_pca_train = mean_squared_error(X_train, Y_train)\n",
    "    var_exp_train = np.sum(pca.explained_variance_ratio_)\n",
    "    dict_e_pca_train[h] = e_pca_train\n",
    "    dict_var_exp_train[h] = var_exp_train\n",
    "    print(\"SKL PCA, h =\", h, \"PCs, reconstruction error:\", str(round(e_pca_train, 3)),\n",
    "          \"total ratio of variance explained:\", str(round(var_exp_train, 3)))\n",
    "    Y_mat_train = Y_train.reshape(n_samples_train, n, n)\n",
    "    n_examples = 10\n",
    "    fig = plt.figure(figsize = (12, 4))\n",
    "    gs = gridspec.GridSpec(nrows = 2, ncols = n_examples, left = 0.1, bottom = 0.25, \n",
    "                           right = 0.95, top = 0.95, wspace = 0.0, hspace = 0.0)\n",
    "    for i in range(0, n_examples):\n",
    "        ax1 = plt.subplot(gs[i])\n",
    "        plt.imshow(X_mat_train[i,:,:], cmap = \"gray\")\n",
    "        ax1.set(adjustable = \"datalim\", aspect = \"auto\")\n",
    "        ax1.get_xaxis().set_visible(False)\n",
    "        ax1.get_yaxis().set_visible(False)\n",
    "        ax2 = plt.subplot(gs[n_examples+i])\n",
    "        plt.imshow(Y_mat_train[i,:,:], cmap = \"gray\")\n",
    "        ax2.set(adjustable = \"datalim\", aspect = \"auto\")\n",
    "        ax2.get_xaxis().set_visible(False)\n",
    "        ax2.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, we see that results improve with increasing $h$ and that the reconstructed images start to look decent for $h = 20$. For $h = d$, the original image, the representation in the embedded space and the reconstruction are all the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A PCA with $h = 2$ allows to obtain a two dimensional representation of our data that we can then plot.\n",
    "It is an interesting, easy and quick way (although slightly naive too!) to see how the data is distributed in this new space, whether clusters might be present or not, etc.\n",
    "Let us apply this technique to the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2)\n",
    "pca.fit(X0_train)\n",
    "Z0_train_pca = pca.transform(X0_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have good looking figures, we define a bunch of `matplotlib` options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_digits = 10\n",
    "fs = 20\n",
    "plt.rc('axes', facecolor = \"white\", linewidth = 1, grid = False, edgecolor = \"black\",\n",
    "       titlesize = fs, labelsize = fs)\n",
    "plt.rc('font', size = fs)\n",
    "plt.rc('xtick', labelsize = fs)\n",
    "plt.rc('ytick', labelsize = fs)\n",
    "plt.rc('legend', fontsize = fs)\n",
    "plt.rc('figure', titlesize = fs, figsize = (15, 10))\n",
    "colors = plt.cm.jet(np.linspace(0, 1, n_digits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each digit $k$ between 0 and 9, we find the indices in our training dataset which corresponds to $k$, and plot the representation of $n_{points} = 500$ points (so as not to overload the image) corresponding to this digit.\n",
    "\n",
    "All instances of a given digit $k$ are displayed in the same color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_points = 500\n",
    "\n",
    "fig = plt.figure()\n",
    "for digit in range(0, 10):\n",
    "    ind = np.where(l_train == digit)\n",
    "    ind = ind[0][0:n_points]\n",
    "    plt.scatter(Z0_train_pca[ind,0], Z0_train_pca[ind,1], \n",
    "                label = str(digit), color =  colors[digit], alpha = 0.7, s = 150)\n",
    "    plt.xlabel(\"PCA 1\")\n",
    "    plt.ylabel(\"PCA 2\")\n",
    "plt.legend(bbox_to_anchor=(1.12, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most instances of a given digit $k$ are grouped together, although there exist (possibly strong) overlaps between different groups of digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another technique worth mentioning to display data in 2 dimensions is t-distributed Stochastic Neighbor Embedding (t-SNE).\n",
    "First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked while dissimilar points have an extremely small probability of being picked.\n",
    "It defines a similar probability distribution over the points in the low-dimensional map, and it then minimizes the Kullback–Leibler divergence between the two distributions with respect to the locations of the points in the map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it can be numerically expensive, we only compute it on a subset of the training set of size `n_samples_tsne = 2000`. We also compute the required time to compute it using the `time` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.manifold import TSNE\n",
    "r_tsne = TSNE(n_components = 2, perplexity = 50.0, early_exaggeration = 20.0)\n",
    "n_samples_tsne = 2000 ## computing t-SNE over the whole dataset is rather costly\n",
    "ti_tsne = time.time()\n",
    "Z0_train_tsne = r_tsne.fit_transform(X0_train[0:n_samples_tsne,:])\n",
    "t_tsne = time.time() - ti_tsne\n",
    "print(\"Time TSNE with\", n_samples_tsne, \"samples:\", str(round(t_tsne, 2)), \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the same kind of figure as for PCA.\n",
    "You can play with the parameters of `perplexity` and `early_exaggeration` to see how it changes the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for digit in range(0, 10):\n",
    "    ind = np.where(l_train[0:n_samples_tsne] == digit)\n",
    "    plt.scatter(Z0_train_tsne[ind,0], Z0_train_tsne[ind,1], label = str(digit), color =  colors[digit], alpha = 0.7, s = 150)\n",
    "    plt.xlabel(\"t-SNE 1\")\n",
    "    plt.ylabel(\"t-SNE 2\")\n",
    "plt.legend(bbox_to_anchor=(1.12, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groups of digits appear to be much better separated than for PCA.\n",
    "In this supervised case, where digit labels are available for confirmation, t-SNE appears very relevant.\n",
    "In general, one must remain careful though:\n",
    "while t-SNE plots often seem to display clusters, the visual clusters can be influenced strongly by the chosen parameterization and therefore a good understanding of the parameters for t-SNE is necessary. Such \"clusters\" can be shown to even appear in non-clustered data, and thus may be false findings. Interactive exploration may thus be necessary to choose parameters and validate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us recall that with a PCA we learn a representation as:\n",
    "\n",
    "$Y^0_{train} = g(Z^0_{train}) = Z^0_{train} V_h$,\n",
    "\n",
    "i.e.:\n",
    "\n",
    "$Y^0_{train} = g(f(X^0_{train})) = X^0_{train} V_h^T V_h$.\n",
    "\n",
    "Can we find other functions $f$ and $g$ so that the reconstruction error:\n",
    "\n",
    "<center>$\\epsilon(X) = \\frac{1}{n_{samples}} \\sum_{i=1}^{n_{samples}} \\frac{1}{d} \\sum_{j=1}^d | X_{ij} - Y_{ij} |^2$.</center>\n",
    "\n",
    "is even lower? This was indeed our a priori criterion to learn a good representation in the space of dimension $h$.\n",
    "\n",
    "An autoencoder is a type of neural network that is comprised of two parts:\n",
    "- an encoder, that _encodes_ the original data in a space of reduced dimension (this is our representation),\n",
    "- a decoder, that _decodes_ the representation back in the original space.\n",
    "\n",
    "Note that this is precisely what we have done so far with PCA, which is indeed equivalent to a particular architecture of an autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder is nothing more than a neural network with layers of weights and activation functions.\n",
    "If $x$ is an input vector (let us recall that in our case this is a vectorized image), the most simple autoencoder can be written in the following way:\n",
    "- encoder: $x \\rightarrow z = f(x) = \\sigma_1(W_1 x)$,\n",
    "- decoder: $z \\rightarrow y = g(z) = \\sigma_2(W_2 x)$.\n",
    "\n",
    "where $W_1$ and $\\sigma_1$ (resp. $W_2$ and $\\sigma_2$) are the weight matrix and the activation function of the first [encoder] layer (resp. the second [decoder] layer). The output of the autoencoder is then simply: $y = g(f(x)) = \\sigma_2(W_2 \\sigma_1 (W_1 x))$.\n",
    "\n",
    "If we choose $W_1 = W_2^T = V_h^{PCA}$ and $\\sigma_1 = \\sigma_2 = Id$, we are left with nothing but the PCA model.\n",
    "However, there may be better choices of $W_1$, $W_2$, $\\sigma_1$ and $\\sigma_2$ to minimize the reconstruction error $\\epsilon$.\n",
    "\n",
    "The strength of neural networks is that they are nonlinear thanks to the (usually nonlinear) activation functions.\n",
    "In our case, once activation functions $\\sigma_1$ and $\\sigma_2$ are chosen, we can optimize the weights of the autoencoder so as to minimize the reconstruction error, i.e. we solve:\n",
    "\n",
    "<center>$W_1, W_2 = \\arg \\min \\frac{1}{n_{samples}} \\sum_{i=1}^{n_{samples}} \\frac{1}{d} \\sum_{j=1}^d | X_{ij} - Y_{ij} |^2$.</center>\n",
    "\n",
    "where $Y = \\sigma_2(W_2 \\sigma_1 (W_1 X))$. This is done using backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refined autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build autoencoders with more complicated architectures than the simple one we just described.\n",
    "For instance, we might want to increase the depth of our network by adding other layers to both the encoder and decoder parts and the dimension of each layer need not be the same. It will comprise more layers, hence more weight matrices, but the principle remains the same:\n",
    "\n",
    "<center>$\\{W_l\\}_{1 \\leq l \\leq L} = \\arg \\min \\frac{1}{n_{samples}} \\sum_{i=1}^{n_{samples}} \\frac{1}{d} \\sum_{j=1}^d | X_{ij} - Y_{ij} |^2$,</center>\n",
    "\n",
    "where $L$ is the number of layers of the model and $Y = \\sigma_L(W_L \\dots \\sigma_1(W_1 X))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build an autoencoder with the following architecture:\n",
    "- a layer with 512 units and activation function `elu`, followed by\n",
    "- a layer with 128 units and activation function `elu`, followed by\n",
    "- a layer with 2 units and a `linear` activation function, followed by\n",
    "- a layer with 128 units and activation function `elu`, followed by\n",
    "- a layer with 512 units and activation function `elu`, followed by\n",
    "- a layer with 784 units and activation function `sigmoid`.\n",
    "  \n",
    "The last layer gives us the reconstructed data in the original space. Notice that we use a `sigmoid` function in order to have output data between 0 and 1, as for the input data (recall that we have scaled it in the very beginning of this notebook).\n",
    "\n",
    "We use `keras` to build these autoencoders.\n",
    "We first define a `Sequential()` `model` and `add` to it different (`Dense`) layers.\n",
    "\n",
    "The last step of defining a `keras` model is to compile it by specifying the objective function, i.e. the loss, which is here the `mean_squared_error` between the input and the output of the model, i.e. precisely:\n",
    "\n",
    "<center>$\\arg \\min \\frac{1}{n_{samples}} \\sum_{i=1}^{n_{samples}} \\frac{1}{d} \\sum_{j=1}^d | X_{ij} - Y_{ij} |^2$.</center>\n",
    "\n",
    "We also need to specify which optimizer we choose, `Adam()` in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation = \"elu\", input_shape = (784,)))\n",
    "model.add(Dense(128, activation = \"elu\"))\n",
    "model.add(Dense(2, activation = \"linear\", name = \"representation\"))\n",
    "model.add(Dense(128, activation = \"elu\"))\n",
    "model.add(Dense(512, activation = \"elu\"))\n",
    "model.add(Dense(784, activation = \"sigmoid\"))\n",
    "model.compile(loss = \"mean_squared_error\", optimizer = Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a summary of the model : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we fit it onto our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, X_train, batch_size = 128, epochs = 5, verbose = 1,\n",
    "                    validation_data = (X_test, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just (hopefully) successfully trained our first autoencoder. We may want to visualize the training history and the representation we have learnt, but if we want to study different architectures of autoencoders, we need an efficient and concise way of defining them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building autoencoders in a generic and efficient way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to study different kinds of architectures in an efficient way, we are going to store once again all our results inside dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_model = {}\n",
    "dict_history = {}\n",
    "dict_encoder = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write a function `build_autoencoder` which takes as an input the string representing the architecture of our autoencoder. These strings will be the keys of the dictionaries. For instance `\"512,elu-128,elu-2,Rlinear-128,elu-512,elu-784,sigmoid\"` will build the architecture we just described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the activation function of a layer starts with `R`, we drop this `R` from the name of the activation function and name the layer `representation`, this will allow us later to easily retrieve the output of this particular layer since we are interested in the representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_arch(arch):\n",
    "    layers = arch.split(\"-\")\n",
    "    return [layer.split(\",\") for layer in layers]\n",
    "\n",
    "def build_autoencoder(arch, d):\n",
    "    arch_split = split_arch(arch)\n",
    "    layer0 = arch_split[0]\n",
    "    model = Sequential()\n",
    "    if layer0[1].startswith(\"R\"):\n",
    "        model.add(Dense(int(layer0[0]), activation = layer0[1][1:], \n",
    "                        name = \"representation\", input_shape = (d,)))\n",
    "    else:\n",
    "        model.add(Dense(int(layer0[0]), activation = layer0[1], input_shape = (d,)))\n",
    "    for i in range(1, len(arch_split)):\n",
    "        layer = arch_split[i]\n",
    "        if layer[1].startswith(\"R\"):\n",
    "            model.add(Dense(int(layer[0]), activation = layer[1][1:], \n",
    "                            name = \"representation\"))\n",
    "        else:\n",
    "            model.add(Dense(int(layer[0]), activation = layer[1]))\n",
    "    model.compile(loss = 'mean_squared_error', optimizer = Adam())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For different values of $h$, we define both a naive architecture of an autoencoder (with only 3 layers in total) and a more refined architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "archs = []\n",
    "hs = [2, 20]\n",
    "for h in hs:\n",
    "    archs.append(str(h) + \",Rlinear-784,sigmoid\")\n",
    "    archs.append(\"512,elu-128,elu-\" + str(h) + \",Rlinear-128,elu-512,elu-784,sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train all these autoencoders for 20 epochs and store their training history.\n",
    "\n",
    "It actually takes some time to obtain all results, you can increase the number of dimensions for the representation space, say `hs = [2, 5, 10, 20, 50, 100, 784]`, and increase the number of epochs to 100 if you want to see the complete training, but you may want to go for lunch while it runs to wait for the results.\n",
    "\n",
    "Note that for refined autoencoders, there are many more weight parameters to learn, the training time is therefore much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "## the full training can take ~15 minutes\n",
    "\n",
    "ti_training = time.time()\n",
    "for arch in archs:\n",
    "    print(arch)\n",
    "    dict_model[arch] = build_autoencoder(arch, d)\n",
    "    dict_history[arch] = dict_model[arch].fit(X_train, X_train, batch_size = 128, \n",
    "                                              epochs = n_epochs, verbose = 1, \n",
    "                                              validation_data = (X_test, X_test))\n",
    "t_training = time.time() - ti_training\n",
    "print(\"Time for training autoencoders:\", t_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each size of the representation, we plot the training curves of both the simple autoencoders and the refined one. We also add to the plot the value of the PCA reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "i = 0\n",
    "for h in hs:\n",
    "    ci = colors[np.mod(4*i, len(colors))]\n",
    "    arch = str(h) + \",Rlinear-784,sigmoid\"\n",
    "    history = dict_history[arch]\n",
    "    plt.plot(history.history['loss'], c = ci, label = str(h) + \" train\")\n",
    "    plt.plot(history.history['val_loss'], c = ci, label = str(h) + \" test\", linestyle = \"--\")\n",
    "    plt.axhline(dict_e_pca_train[h], c = ci)\n",
    "    arch = \"512,elu-128,elu-\" + str(h) + \",Rlinear-128,elu-512,elu-784,sigmoid\"\n",
    "    history = dict_history[arch]\n",
    "    plt.plot(history.history['loss'], c = ci, marker = \"*\", label = str(h) + \" R train\")\n",
    "    plt.plot(history.history['val_loss'], c = ci,  label = str(h) + \" R test\", marker = \"s\")\n",
    "    i += 1\n",
    "plt.xlim(0, n_epochs-1)\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(bbox_to_anchor = (1.02, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two main things can be noted:\n",
    "- the autoencoders (even simple ones) almost always outperform the PCA for a sufficiently large number of epochs,\n",
    "- the refined autoencoders are much more efficient than the simple ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of the 2D representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now display the representation in 2D in the following case:\n",
    "  - the PCA,\n",
    "  - t-SNE,\n",
    "  - the simple autoencoder,\n",
    "  - the refined autoencoder,\n",
    "  \n",
    "in order to see how the data (and groups of digits) spread in the 2 dimensional representation space in each case.\n",
    "\n",
    "To obtain the representation of an autoencoder, we define a new `Model()` which takes the same input as the autoencoder, and use `get_layer().output` to obtain the output of the layer named `representation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_plot = 100\n",
    "\n",
    "plt.figure(figsize = (15, 15))\n",
    "gs = gridspec.GridSpec(nrows = 2, ncols = 2, left = 0.1, bottom = 0.25, right = 0.95, top = 0.95,\n",
    "                       wspace = 0.2, hspace = 0.2)\n",
    "\n",
    "## PCA\n",
    "ax1 = plt.subplot(gs[0])\n",
    "for digit in range(0, 10):\n",
    "    ind = np.where(l_train == digit)\n",
    "    ind = ind[0][0:n_samples_plot]\n",
    "    pca = dict_pca[2]\n",
    "    Z0_train_pca = pca.transform(X0_train)\n",
    "    plt.scatter(Z0_train_pca[ind,0], Z0_train_pca[ind,1], label = str(digit), color =  colors[digit], alpha = 0.7, s = 150)\n",
    "ax1.title.set_text(\"PCA\")\n",
    "\n",
    "## t-SNE\n",
    "ax2 = plt.subplot(gs[1])\n",
    "for digit in range(0, 10):\n",
    "    ind = np.where(l_train == digit)\n",
    "    ind = ind[0][0:n_samples_plot]\n",
    "    plt.scatter(Z0_train_tsne[ind,0], Z0_train_tsne[ind,1], label = str(digit), color =  colors[digit], alpha = 0.7, s = 150)\n",
    "ax2.title.set_text(\"t-SNE\")\n",
    "\n",
    "## Simple AE\n",
    "arch = \"2,Rlinear-784,sigmoid\"\n",
    "dict_encoder[arch] = Model(dict_model[arch].input, dict_model[arch].get_layer(\"representation\").output)\n",
    "Z_train_ = dict_encoder[arch].predict(X_train)\n",
    "ax3 = plt.subplot(gs[2])\n",
    "for digit in range(0, 10):\n",
    "    ind = np.where(l_train == digit)\n",
    "    ind = ind[0][0:n_samples_plot]\n",
    "    plt.scatter(Z_train_[ind,0], Z_train_[ind,1], label = str(digit), color =  colors[digit], alpha = 0.7, s = 150)\n",
    "ax3.title.set_text(\"Simple AE\")\n",
    "\n",
    "## Refined AE\n",
    "arch = \"512,elu-128,elu-2,Rlinear-128,elu-512,elu-784,sigmoid\"\n",
    "dict_encoder[arch] = Model(dict_model[arch].input, dict_model[arch].get_layer(\"representation\").output)\n",
    "Z_train_ = dict_encoder[arch].predict(X_train)\n",
    "ax4 = plt.subplot(gs[3])\n",
    "for digit in range(0, 10):\n",
    "    ind = np.where(l_train == digit)\n",
    "    ind = ind[0][0:n_samples_plot]\n",
    "    plt.scatter(Z_train_[ind,0], Z_train_[ind,1], label = str(digit), color =  colors[digit], alpha = 0.7, s = 150)\n",
    "ax4.title.set_text(\"Refined AE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can visualize the reconstruction of our images to assess how well they are reconstructed.\n",
    "For each $h$, we display:\n",
    "- the original image,\n",
    "- the PCA reconstruction,\n",
    "- the simple autoencoder reconstruction,\n",
    "- the refined autoencoder reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 10\n",
    "\n",
    "for h in hs:\n",
    "    ## PCA\n",
    "    pca = dict_pca[h]\n",
    "    Z0_train = pca.transform(X0_train[:n_examples])\n",
    "    Y0_train_pca_ex = pca.inverse_transform(Z0_train)\n",
    "    Y_train_pca_ex = Y0_train_pca_ex + m_train\n",
    "    Y_mat_train_pca_ex = Y_train_pca_ex.reshape(n_examples, n, n)\n",
    "    ## Basic AE\n",
    "    arch = str(h) + \",Rlinear-784,sigmoid\"\n",
    "    Y_train_bae_ex = dict_model[arch].predict(X_train[0:n_examples,:])\n",
    "    Y_mat_train_bae_ex = Y_train_bae_ex.reshape(n_examples, n, n)\n",
    "    ## Refined AE\n",
    "    arch = \"512,elu-128,elu-\" + str(h) + \",Rlinear-128,elu-512,elu-784,sigmoid\"\n",
    "    Y_train_rae_ex = dict_model[arch].predict(X_train[0:n_examples,:])\n",
    "    Y_mat_train_rae_ex = Y_train_rae_ex.reshape(n_examples, n, n)\n",
    "\n",
    "    fig = plt.figure(figsize = (12, 8))\n",
    "    gs = gridspec.GridSpec(nrows = 4, ncols = n_examples, left = 0.1, bottom = 0.25, right = 0.95, top = 0.95,\n",
    "                           wspace = 0.0, hspace = 0.0)\n",
    "    for i in range(0, n_examples):\n",
    "        ## Real\n",
    "        ax1 = plt.subplot(gs[i])\n",
    "        plt.imshow(X_mat_train[i,:,:], cmap = \"gray\")\n",
    "        ax1.set(adjustable = \"datalim\", aspect = \"auto\")\n",
    "        ax1.get_xaxis().set_visible(False)\n",
    "        ax1.get_yaxis().set_visible(False)\n",
    "        fig.text(0.07, 0.87, \"Real\", va = \"center\", rotation = \"vertical\")\n",
    "        ## PCA\n",
    "        ax2 = plt.subplot(gs[n_examples+i])\n",
    "        plt.imshow(Y_mat_train_pca_ex[i,:,:], cmap = \"gray\")\n",
    "        ax2.set(adjustable = \"datalim\", aspect = \"auto\")\n",
    "        ax2.get_xaxis().set_visible(False)\n",
    "        ax2.get_yaxis().set_visible(False)\n",
    "        fig.text(0.07, 0.69, \"PCA\", va = \"center\", rotation = \"vertical\")\n",
    "        ## Basic AE\n",
    "        ax3 = plt.subplot(gs[2*n_examples+i])\n",
    "        plt.imshow(Y_mat_train_bae_ex[i,:,:], cmap = \"gray\")\n",
    "        ax3.set(adjustable = \"datalim\", aspect = \"auto\")\n",
    "        ax3.get_xaxis().set_visible(False)\n",
    "        ax3.get_yaxis().set_visible(False)\n",
    "        fig.text(0.07, 0.51, \"Basic AE\", va = \"center\", rotation = \"vertical\")\n",
    "        ## Refined AE\n",
    "        ax4 = plt.subplot(gs[3*n_examples+i])\n",
    "        plt.imshow(Y_mat_train_rae_ex[i,:,:], cmap = \"gray\")\n",
    "        ax4.set(adjustable = \"datalim\", aspect = \"auto\")\n",
    "        ax4.get_xaxis().set_visible(False)\n",
    "        ax4.get_yaxis().set_visible(False)\n",
    "        fig.text(0.07, 0.33, \"Ref. AE\", va = \"center\", rotation = \"vertical\")\n",
    "    plt.suptitle(\"h = \" + str(h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again notice that:\n",
    "- reconstructed images are much better with autoencoders than PCA,\n",
    "- reconstructed images of the refined autoencoders are much better than those of the simple one, whence the importance of designing a good architecture of our autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to tackle the problem of clustering our representations to try to identify meaningful groups.\n",
    "Usually we do not know how many clusters there are; here we are going to assume that we have $K = 10$ clusters.\n",
    "\n",
    "We are going to use $K$-means to do so and initialize our centroids with the `kmeans++` method.\n",
    "Note that we could also initialize randomly our centroids which might yield slightly better results in terms of clustering indices. You can try to set `n_random = 10` to launch $K$-means with such an initialization and see the results it yields.\n",
    "\n",
    "For each architecture of our autoencoders, we are going to cluster the representation of our data and store the results in a dictionary `dict_kmeans`. We will also compute two internal clustering indices: the silhouette score and the Davies-Bouldin score. They will be stored in a `DataFrame` (using the `pandas` module) `df` that will sum up the clustering results for each architecture. Note that two different values of a score can be compared only if they are computed on the same data, i.e. the same representation. `DataFrames` are very practical for this task since you can then filter your results based on a condition (a given architecture, a given dimension of the representation, etc.) or find the maximum value of a given variable (e.g. the Davies-Bouldin index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import pandas as pd\n",
    "\n",
    "columns = [\"arch\", \"h\", \"k\", \"init\", \"silhouette\", \"davies_bouldin\"]\n",
    "df = pd.DataFrame(columns = columns)\n",
    "\n",
    "dict_kmeans = {}\n",
    "n_random = 1              # number of different random initializations\n",
    "n_samples_kmeans = 10000    # set to 60000 if you want to take into account all data (longer training)\n",
    "range_k = range(10, 11)     # we study results for k = 10 only, feel free to investigate further, range(5, 15) for instance\n",
    "\n",
    "## the training of each architecture usually takes between 1 and 2 minutes with all data and n_random = 0\n",
    "\n",
    "for h in hs:\n",
    "    print(\"h:\", h)\n",
    "    for arch in [str(h) + \",Rlinear-784,sigmoid\",\n",
    "                 \"512,elu-128,elu-\" + str(h) + \",Rlinear-128,elu-512,elu-784,sigmoid\"]:\n",
    "        print(\"  arch:\", arch)\n",
    "        ti_clustering = time.time()\n",
    "        dict_encoder[arch] = Model(dict_model[arch].input,\n",
    "                                   dict_model[arch].get_layer(\"representation\").output)\n",
    "        Z_train = dict_encoder[arch].predict(X_train)\n",
    "        Z_train = Z_train[0:n_samples_kmeans,:]\n",
    "        dict_kmeans[arch] = {}\n",
    "        for k in range_k:\n",
    "            dict_kmeans[arch][k] = KMeans(n_clusters = k).fit(Z_train)\n",
    "            sil = silhouette_score(Z_train, dict_kmeans[arch][k].labels_)\n",
    "            db = davies_bouldin_score(Z_train, dict_kmeans[arch][k].labels_)\n",
    "            df = df.append({\"arch\": arch, \"h\": h, \"k\": k, \n",
    "                            \"init\": \"kmeans++\", \"silhouette\": sil, \n",
    "                            \"davies_bouldin\": db},\n",
    "                           ignore_index = True)\n",
    "            for l in range(0, n_random):\n",
    "                km = KMeans(n_clusters = k, init = \"random\").fit(Z_train)\n",
    "                sil = silhouette_score(Z_train, km.labels_)\n",
    "                db = davies_bouldin_score(Z_train, km.labels_)\n",
    "                df = df.append({\"arch\": arch, \"h\": h, \"k\": k, \n",
    "                                \"init\": \"random\", \"silhouette\": sil, \n",
    "                                \"davies_bouldin\": db},\n",
    "                               ignore_index = True)\n",
    "        t_clustering = time.time() - ti_clustering\n",
    "        print(\"Clustering time:\", t_clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering results stored in `df` are now available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could define functions to retrieve only the lines that correspond, for instance, to a particular architecture and a number of clusters. We could also find the case for which the silhouette (resp. Davies-Bouldin) index is maximal (resp. minimal). This is useful if we have several clustering algorithms for the same case (i.e. initializing $K$-means with different initializations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select(df, arch, k): return df[(df[\"arch\"] == arch) & (df[\"k\"] == k)]\n",
    "def silhouette_max(df): return df.loc[df[\"silhouette\"].idxmax()].silhouette\n",
    "def davies_bouldin_min(df): return df.loc[df[\"davies_bouldin\"].idxmin()].davies_bouldin\n",
    "\n",
    "arch = \"2,Rlinear-784,sigmoid\"\n",
    "k = 10\n",
    "df_filtered = select(df, arch, k)\n",
    "sil_max = silhouette_max(df_filtered)\n",
    "db_min = davies_bouldin_min(df_filtered)\n",
    "\n",
    "print(df_filtered)\n",
    "print(\"\\n\")\n",
    "print(\"Maximum silhouette score for {} : {:.3f}\".format(arch, sil_max))\n",
    "print(\"Minimum Davies-Bouldin score for {} : {:.3f}\".format(arch, db_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also display the evolution of the clustering indices with the number of clusters. This will obviously work only if you have set `range_k` to include several values in the clustering step above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in hs:\n",
    "    for arch in [str(h) + \",Rlinear-784,sigmoid\",\n",
    "                 \"512,elu-128,elu-\" + str(h) + \",Rlinear-128,elu-512,elu-784,sigmoid\"]:\n",
    "        vec_k = np.zeros(0)\n",
    "        vec_sil = np.zeros(0)\n",
    "        vec_db = np.zeros(0)\n",
    "        for k in range_k:\n",
    "            vec_k = np.append(vec_k, k)\n",
    "            vec_sil = np.append(vec_sil, silhouette_max(select(df, arch, k)))\n",
    "            vec_db = np.append(vec_db, davies_bouldin_min(select(df, arch, k)))\n",
    "        plt.figure(figsize = (15, 5))\n",
    "        plt.plot(vec_k, vec_db)\n",
    "        plt.xlabel(\"Number of clusters\")\n",
    "        plt.ylabel(\"Davies-Bouldin index\")\n",
    "        plt.title(arch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plot the evolution of the indices according to the number of clusters, you will not see an obvious maximum silhouette or minimum DB for $k=10$. As illustrated by the 2D representations above, the clusters do not separate easily. \n",
    "\n",
    "Clustering is a rather hard problem, and having to cluster data from representations may yield clusters composed of different types of objects. Here, since the MNIST dataset is labelled, we can investigate the different allocations of digits within the clusters that we have found, and see which digits are harder to discriminate.\n",
    "\n",
    "We are going to investigate, for each cluster $0 \\leq c \\leq 9$ and digit $0 \\leq d \\leq 9$, how many digits $d$ we have found in cluster $c$, and store the results inside a `DataFrame` `df_allocation`.\n",
    "\n",
    "This will provide us with raw counts of digits per cluster. We also define another `DataFrame` `df_allocation_relative` in which we divide each number of digits $d$ in cluster $c$ by the total number of digits in cluster $c$ so as to obtain the proportions of each digit in a given cluster.\n",
    "\n",
    "Results can then be printed or displayed as heatmaps using the `seaborn` module, which makes it visually easy to understand of what digits the different clusters are mainly made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "dict_df_allocation = {}\n",
    "dict_df_allocation_relative = {}\n",
    "\n",
    "list_digits = list(map(lambda d: str(d), range(0,10)))\n",
    "\n",
    "for h in hs:\n",
    "    for arch in [str(h) + \",Rlinear-784,sigmoid\",\n",
    "                 \"512,elu-128,elu-\" + str(h) + \",Rlinear-128,elu-512,elu-784,sigmoid\"]:\n",
    "        labels = dict_kmeans[arch][10].labels_ ## we focus on the case k = 10\n",
    "        df_allocation = pd.DataFrame(columns = [\"cluster\"] + list_digits + [\"total\"])\n",
    "        for c in range(0, 10):\n",
    "            ind = np.where(labels == c)\n",
    "            counts = np.zeros(10, dtype = int)\n",
    "            for d in range(0, 10):\n",
    "                counts[d] = np.where(l_train[ind] == d)[0].size\n",
    "            df_allocation.loc[c] = np.concatenate([c, counts, np.sum(counts)], axis = None)\n",
    "        dict_df_allocation[arch] = df_allocation\n",
    "        df_allocation_relative = df_allocation.copy()\n",
    "        for c in range(0, 10):\n",
    "            for d in list_digits:\n",
    "                df_allocation_relative[d][c] /= df_allocation[\"total\"][c]\n",
    "            df_allocation_relative[\"total\"][c] = np.sum(df_allocation_relative[list_digits].iloc[c])\n",
    "        dict_df_allocation_relative[arch] = df_allocation_relative\n",
    "        for digit in list_digits:\n",
    "            df_allocation_relative[digit] = df_allocation_relative[digit].apply(lambda x: str(round(x, 2)))\n",
    "        print(\"Arch:\", arch)\n",
    "        print(\"\\n\")\n",
    "        print(df_allocation_relative)\n",
    "        print(\"\\n\")\n",
    "        data = dict_df_allocation_relative[arch][list_digits].values.astype(float)\n",
    "        fig = plt.figure(figsize = (8, 6))\n",
    "        sns.heatmap(data)\n",
    "        plt.ylabel(\"Clusters\")\n",
    "        plt.xlabel(\"Digits\")\n",
    "        plt.suptitle(arch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then identify if each cluster (i.e. each row) is composed of more digits than some others. (Obviously, by default, there is no reason for the cluster index to correspond to the main digit in it.)\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
